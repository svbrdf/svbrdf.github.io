<!DOCTYPE HTML>
<html xmlns:v="urn:schemas-microsoft-com:vml" xmlns:o="urn:schemas-microsoft-com:office:office">
<head>
<meta charset="utf-8">
<title>Hongzhi Wu</title>
<link href="gly.css" rel="stylesheet" type="text/css">


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DVPDFQYVF2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DVPDFQYVF2');
</script>


<style type="text/css">
.style2 {
	font-size: 10pt;
}
.style3 {
	font-family: Georgia, Times, serif;
	font-size: 18pt;
	font-style: normal;
	font-weight: bold;
	color: #000000;
}
.style4 {
				background-color: #FFFFFF;
}
.style5 {
				text-align: center;
}
.style6 {
	color: rgb(255, 0, 0);
}
.style7 {
	text-decoration: underline;
}
</style>
</head>

<body>

<div class="container">      
    <table width="95%" border="0" align="center">
      <tr>
        <td><div align="center">
			<img src="./images/hwu.png" width="135" align="middle"  style="float: left"></div></td>
        <td width="52%"><div align="center">
          <table width="94%" border="0" style="height: 180px">
            <tr>
            <td width="19%" valign="top" style="height: 53px"></td>
            <td width="81%" valign="top" style="height: 53px">
              <table width="95%" border="0" align="left">
                <tr>
                  <td align="center"><span class="style3">Hongzhi (Rick) Wu</span><br><a href="./images/name.mp3" target="_new">
					<font size="2">(pronunciation)</font></a></td>
                </tr>
              </table></td>          
            <tr>
            <td width="19%" valign="top" class="style2" style="height: 24px">Address:</td>
            <td width="81%" valign="top" style="height: 24px" class="style2">B425 Mengminwei Bldg.<br>
			Zijingang Campus<br>
			Zhejiang University<br>
			866 Yuhangtang Rd.<br>
			Hangzhou, 310058, China</td>
          </tr>
          <tr>
            <td height="22" class="style2">Email:</td>
            <td class="style2">hwu at acm dot org</td>
          </tr>
          </table>
        </div></td>
        <td width="28%"><div align="center">
          <table width="100%" border="0">
            <tr>
              <td align="center"><a href="http://www.zju.edu.cn/english/"><img src="images/zju_logo.png" width="75" height="75"></a></td>
              <td align="center"><a href="http://www.yale.edu/"><img src="images/yale_logo.jpg" width="75" height="75"></a></td>
            </tr>
            <tr>
              <td align="center"><a href="http://research.microsoft.com/aboutmsr/labs/asia/"><img src="images/msra_logo.png" width="75" height="62"></a></td>
              <td align="center"><a href="http://www.fudan.edu.cn/englishnew/"><img src="images/fudan_logo.jpg" width="75" height="75"></a></td>
            </tr>
          </table>
        </div></td>
      </tr>
  </table>


	<p><br>
  I am a professor (教授/博导) in the <a href="http://www.cad.zju.edu.cn/english.html">State Key Lab of CAD&amp;CG</a>, <a href="http://www.zju.edu.cn/english/">Zhejiang University</a>. I am also a recipient of Excellent Young Scholars (国家优青), NSF-China.
	My research interest lies in the acquisition and reconstruction of physical information, including complex high-dimensional appearance, 3D surfaces and volumes.</p>
	<p>I received my Ph.D. from <a href="http://www.cs.yale.edu/">Dept. of Computer Science</a>,
<a href="http://www.yale.edu/">Yale University</a>, under the supervision by Prof. <a href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a> and Prof. <a href="http://graphics.cs.yale.edu/holly/">
Holly Rushmeier</a>. Prior to that, I obtained my B.Sc. from Dept. of Computer Science and Engineering, <a href="http://en.wikipedia.org/wiki/Fudan_University">Fudan University</a>. I was 
	also a visiting student in <a href="http://research.microsoft.com/ig/">Internet Graphics Group</a> at <a href="http://research.microsoft.com/aboutmsr/labs/asia/">Microsoft Research Asia</a>.</p>
	<p>&nbsp;</p>

<p class="textsectionheader2">Professional Services</p>
Editorial Board of IEEE TVCG, Journal of CAD&CG and VCIBA.<br> 
Program Committee Member for SIGGRAPH/SIGGRAPH Asia, EG, VR, EGSR, PG, I3D, HPG, CVM and CAD/Graphics.<br> 
Reviewer for ACM TOG, IEEE TVCG, CGF, Science China, SIGGRAPH/SIGGRAPH Asia, EG, EGSR, PG, AAAI, C&G.<br> 
Conflict-of-Interest(COI) Coordinator for SIGGRAPH/SIGGRAPH Asia.<br>
Reviewer for General Program, NSF China and ISF-NSFC Joint Scientific Research Program.<br> 
Secretary of Program Chair, Chinagraph.<br> 
Secretary for International Cooperation and Exchange Working Committee, China Society of Image and Graphics.
<p><br></p>    

<p class="textsectionheader2">To Prospective Students</p>
	<p>I leverage the knowledge from multiple fields to solve challenging problems in 
	computer graphics and vision. This involves a lot of different cool things, 
	including software &amp; hardware programming, and mechanical design.&nbsp;I am looking for talented students with a strong background in one or more of the following fields:<br>
	1. Computer Graphics and Vision.<br>
  2. Optics. <br> 
	3. Electrical Engineering.<br> 
	4. Mechanics/Robotics. <p>
	I am open to various ways of engaging in my lab, from being an advisor for 
	your master/PhD study, to a research advisor for your undergraduate study, 
	or just doing a summer intern. Please drop me an email to schedule a 
	meeting, if you are interested in working with me.<p><br></p>

<p class="textsectionheader2">Teaching</p>
	<p><a href="cg.html">Computer Graphics</a> (2015-F, 2016-S/F, 2017-S/F, 2018-S/F, 2019-F, 2020-F, 2021-F, 2022-F, 2023-F, 2024-F)<br>
	<p><a href="iavi.html">Intelligent Acquisition of Visual Information</a> (2019-F, 2020-F, 2021-F, 2022-F, 2023-F, 2024-F)<br>
	<span lang="en-us">	
	<p>For students who are unable to register my course due to limited 
	capacity, please make sure that you select the course in the second round of 
	registration (补选) and see me in the first class. Please register 
	the course according to your curriculum (培养计划). For computer science major, please take the 
	course when you are<span lang="en-us"> <strong>at least</strong></span> a 
	junior student (大三<span lang="en-us">), as this course requires the 
	knowledge from a variety of fundamental courses.
	</span></p>
	<p>&nbsp;</p>
	</span>

<p class="textsectionheader2">Selected Publications</p>
<table width="100%" border="0" cellspacing="0">
   <tr>
  </tr>    	

    <tr>
    <td><img src="images/opensubstance.jpg" width="160" height="90" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>OpenSubstance: A High-Quality Measured Dataset of Multi-View and -Lighting Images and Shapes</b><br> 
    Fan Pei,
    Jinchen Bai, 
    <a href="https://sigfeng.com/">Xiang Feng</a>,    
    <a href="https://github.com/RupertPaoZ/">Zoubin Bi</a>,
    <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
    To appear in ICCV 2025.
    <br>
    [Project Page]<br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/picker.jpg" width="160" height="65" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>MaterialPicker: Multi-Modal DiT-Based Material Generation</b><br> 
    <a href="http://xiaohema98.com/">Xiaohe Ma</a>, Valentin Deschaintre, Milos Hasan, Fujun Luan,
    <a href="http://kunzhou.net/">Kun Zhou</a>, <a href="https://svbrdf.github.io/">Hongzhi Wu</a>
    and Yiwei Hu<br>
    To appear in ACM Trans. on Graph. (Proc. SIGGRAPH 2025).
    <br>
    <a href="publications/picker.pdf">[Paper(.PDF), 38.6MB)]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
     <td><img src="images/renderformer.jpg" width="160" height="107" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination</b><br>     
    <a href="https://www.ncj.wiki/">Chong Zeng</a>,
    <a href="https://yuedong.shading.me/">Yue Dong</a>,
    <a href="https://www.cs.wm.edu/~ppeers/">Pieter Peers</a>,    
    <a href="http://hongzhiwu.com/">Hongzhi Wu</a> and
    <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a><br> 
    To appear in ACM SIGGRAPH 2025 Conference Papers.<br>
    <a href="https://microsoft.github.io/renderformer/">[Project Page]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>    

    <tr>
     <td><img src="images/arm.jpg" width="160" height="72" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>ARM: Appearance Reconstruction Model for Relightable 3D Generation</b><br>     
    <a href="https://sigfeng.com/">Xiang Feng</a>, 
    Chang Yu, 
    <a href="https://github.com/RupertPaoZ/">Zoubin Bi</a>, 
    Yintong Shang, Feng Gao,
    <a href="https://svbrdf.github.io">Hongzhi Wu</a>,
    <a href="http://kunzhou.net/">Kun Zhou</a>,
    Chenfanfu Jiang and Yin Yang.<br>
    CVPR 2025 (Highlight).<br>
    <a href="https://arm-aigc.github.io/">[Project Page]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>    

    <tr>
     <td><img src="images/splash.jpg" width="160" height="120" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering</b><br>     
    Yutao Feng, 
    <a href="https://sigfeng.com/">Xiang Feng</a>, 
    Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong,
    Tianjia Shao,
    <a href="https://svbrdf.github.io">Hongzhi Wu</a>,
    <a href="http://kunzhou.net/">Kun Zhou</a>,
    Chenfanfu Jiang and Yin Yang.<br>
    CVPR 2025.<br>
    <a href="https://gaussiansplashing.github.io/">[Project Page]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>    

    <tr>
    <td><img src="images/tripleGS.jpg" width="160" height="94" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>GS<sup>3</sup>: Efficient Relighting with Triple Gaussian Splatting</b><br> 
    <a href="https://github.com/RupertPaoZ/">Zoubin Bi#</a>,
    <a href="https://zyx45889.github.io/">Yixin Zeng#</a>,
    <a href="https://www.ncj.wiki/">Chong Zeng</a>, 
    Fan Pei, Xiang Feng, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
    To appear in ACM SIGGRAPH Asia 2024.
    <br>
    <a href="https://gsrelight.github.io/">[Project Page]</a><br>
    #:contributed equally.
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/ipad.jpg" width="160" height="69" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Learning Photometric Feature Transform for Free-form Object Scan</b><br>     

    <a href="https://sigfeng.com/">Xiang Feng</a>, 
    <a href="http://cocoakang.cn/">Kaizhang Kang</a>, 
    Fan Pei, Huakeng Ding*, Jinjiang You*, 
    <a href="https://ece.hkust.edu.hk/pingtan">Ping Tan</a>,
    <a href="http://kunzhou.net/">Kun Zhou</a> and
    <a href="https://svbrdf.github.io">Hongzhi Wu</a><br>
    To appear in IEEE TVCG.
    <br>
    <a href="publications/ipad/paper.pdf">[Preprint(.PDF), 0.9MB]</a> 
    <a href="publications/ipad/supp.pdf">[Supplemental Material(.PDF), 0.2MB]</a> 
    [Video: <a href="https://youtu.be/YHj-MN9-FMU">Youtube</a>/<a href="https://www.bilibili.com/video/BV1UVqNYFE3W/">Bilibili</a>]<br>
    *:(who) was an undergraduate student when working on this project.
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>    

    <tr>
     <td><img src="images/delightnet.jpg" width="160" height="81" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation</b><br>     
    <a href="https://www.ncj.wiki/">Chong Zeng</a>,
    <a href="https://yuedong.shading.me/">Yue Dong</a>,
    <a href="https://www.cs.wm.edu/~ppeers/">Pieter Peers</a>,    
    <a href="https://github.com/DQSSSSS">Youkang Kong</a>,    
    <a href="http://hongzhiwu.com/">Hongzhi Wu</a> and
    <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a><br> 
     ACM SIGGRAPH 2024 Conference Proceedings, Article 73, 1–12.<br>
    <a href="https://dilightnet.github.io/">[Project Page]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/realtimedynamic.jpg" width="160" height="79" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Real-time Acquisition and Reconstruction of Dynamic Volumes with Neural Structured Illumination</b><br> 
    <a href="https://zyx45889.github.io/">Yixin Zeng#</a>, 
    <a href="https://github.com/RupertPaoZ/">Zoubin Bi#</a>, Mingrui Yin*, Xiang Feng, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
    CVPR 2024, pp. 20186-20195.
    <br>
    <a href="publications/realtimedynamic/project.html">[Project Page]</a><br>
    #:contributed equally.<br>
    *:(who) was an undergraduate student when working on this project.
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/dynamicCT.jpg" width="160" height="120" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Differentiable Dynamic Visible-Light Tomography</b><br> 
    <a href="http://www.cocoakang.cn">Kaizhang Kang#</a>, 
    Zoubin Bi#, Xiang Feng, Yican Dong*, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
    ACM SIGGRAPH Asia 2023 Conference Papers, Article 102, 1–12.
    <br>
    <a href="publications/dynamicCT/project.html">[Project Page]</a><br>
    #:contributed equally.<br>
    *:(who) was an undergraduate student when working on this project.
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/opensvbrdf.jpg" width="160" height="108" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>OpenSVBRDF: A Database of Measured Spatially-Varying Reflectance</b><br> 
    <a href="http://xiaohema98.com/">Xiaohe Ma</a>,
    Xianmin Xu, Leyao Zhang*, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="https://svbrdf.github.io/">Hongzhi Wu</a><br>
    ACM Trans. Graph. (Proc. SIGGRAPH Asia 2023), 42, 6 (Dec. 2023), 254.<br>
    <a href="publications/OpenSVBRDF/project.html">[Project Page]</a>
    <a href="https://www.replicabilitystamp.org/index.html#https-github-com-opensvbrdf-opensvbrdf-source-code">[Graphics Replicability Stamp]</a><br>
    *:(who) was an undergraduate student when working on this project.
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/zebrafish.jpg" width="160" height="87" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Use of Deep-Learning Assisted Assessment of Cardiac Parameters in Zebrafish to Discover Cyanidin Chloride as a Novel Keap1 Inhibitor Against Doxorubicin-Induced Cardiotoxicity</b><br> 
    Changtong Liu, Yingchao Wang, Yixin Zeng, Zirong Kang, Hong Zhao, Kun Qi, <a href="https://svbrdf.github.io/">Hongzhi Wu</a>, <a href="https://person.zju.edu.cn/en/lu_zhao">Lu Zhao</a> and <a href="https://person.zju.edu.cn/en/yiwang_en">Yi Wang</a><br>    
    Advanced Science, 2023, 2301136.<br>
    <a href="https://onlinelibrary.wiley.com/doi/10.1002/advs.202301136">[Publisher Site (Open Access)]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/unified.jpg" width="160" height="68" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>A Unified Spatial-Angular Structured Light for Single-View Acquisition of Shape and Reflectance</b><br>     
    Xianmin Xu#, Yuxin Lin#, Haoyang Zhou*, <a href="https://www.ncj.wiki/">Chong Zeng</a>*, Yaxin Yu, <a href="http://kunzhou.net/">Kun Zhou</a> and <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a><br>
    CVPR 2023, 206-215.
    <br>
    <a href="publications/unified/project.html">[Project Page]</a><br>
    #:contributed equally.<br>     
    *:(who) were undergraduate students when working on this project.    
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  


    <tr>
    <td><img src="images/hints.jpg" width="160" height="107" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Relighting Neural Radiance Fields with Shadow and Highlight Hints</b><br>     
    <a href="https://www.ncj.wiki/">Chong Zeng</a>,
    <a href="https://www.microsoft.com/en-us/research/people/guoch/">Guojun Chen</a>,
    <a href="https://yuedong.shading.me/">Yue Dong</a>,
    <a href="https://www.cs.wm.edu/~ppeers/">Pieter Peers</a>,    
    <a href="http://hongzhiwu.com/">Hongzhi Wu</a> and
    <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a><br> 
    ACM SIGGRAPH 2023 Conference Proceedings, Article 73, 1–11.
    <br>
    <a href="https://nrhints.github.io/">[Project Page]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/moe_arxiv.jpg" width="160" height="120" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Efficient Reflectance Capture with a Deep Gated Mixture-of-Experts</b><br>     
    <a href="http://xiaohema98.com/">Xiaohe Ma</a>,
    Yaxin Yu, 
    <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>+ and
    <a href="http://kunzhou.net/">Kun Zhou</a><br>
    To appear in IEEE TVCG.
    <br>
    <a href="publications/moe/moe.pdf">[Preprint(.PDF), 8.4MB]</a> [Video: <a href="https://youtu.be/v2SyeMUV6_Y">Youtube</a>/<a href="https://www.bilibili.com/video/BV1ZL411r7iq/">Bilibili</a>]<br>
    +:corresponding author.<br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>  

    <tr>
    <td><img src="images/lightview.jpg" width="160" height="101" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Neural Reflectance Capture in the View-Illumination Domain</b><br>     
    <a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
    Minyi Gu,
    Cihui Xie, 
    <a href="https://th3charlie.github.io/aboutme/">Xuanda Yang</a>*,
    <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>+ and
    <a href="http://kunzhou.net/">Kun Zhou</a><br>
    IEEE TVCG, 29, 2(Feb. 2023), 1450-1462.
    <br>
    <a href="publications/lightview/lightview_lq.pdf">[Preprint(.PDF), 6.5MB]</a>
    [Video: <a href="https://youtu.be/JXQGSYux26I">Youtube</a>/<a href="https://www.bilibili.com/video/bv1SP4y187Jr">Bilibili</a>]
    <a href="https://ieeexplore.ieee.org/document/9557800">[IEEE Xplore]</a><br>
    +:corresponding author.<br>     
    *:(who) was an undergraduate student when working on this project.<br>        
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>

    <tr>
    <td><img src="images/dift_arxiv.jpg" width="160" height="79" class="papericon"></td>
    <td> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>DiFT: Differentiable Differential Feature Transform for Multi-View Stereo</b><br>     
	  <a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
	  Chong Zeng*, 
    <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>+ and
	  <a href="http://kunzhou.net/">Kun Zhou</a><br>
	  <a href="https://arxiv.org/abs/2203.08435">[arxiv]</a><br>
    +:corresponding author.<br>
	  *:(who) was an undergraduate student when working on this project.<br> 	      
	  </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>

    <tr>
    <td><img src="images/dd.jpg" width="160" height="103" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields</b><br>     
    Guangming Yao, 
    <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, 
    Yi Yuan,
    Lincheng Li, 
    <a href="http://kunzhou.net/">Kun Zhou</a>
    and Xin Yu<br>
    IJCAI-ECAI 2022.<br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>

    <tr>
    <td><img src="images/multirelight.jpg" width="160" height="90" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
    <td class="papertext"><b>A Multi-Resolution Network Architecture for
Deferred Neural Lighting</b><br>     
    Shengjie Ma, 
    <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, 
    <a href="https://person.zju.edu.cn/en/renzhong">Zhong Ren</a> 
    and
    <a href="http://kunzhou.net/">Kun Zhou</a><br>
    CASA 2022.<br>
    </td>
    <td>&nbsp;</td>
  </tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>

    <tr>
    <td><img src="images/lpft.jpg" width="160" height="107" class="papericon"></td>
    <td> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Learning Efficient Photometric Feature Transform for Multi-view Stereo</b><br>     
	  <a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
	  Cihui Xie, 
	  Ruisheng Zhu, 
    <a href="http://xiaohema98.com/">Xiaohe Ma</a>,
      <a href="https://www.cs.sfu.ca/~pingtan/"> Ping Tan,
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>+ and
	  <a href="http://kunzhou.net/">Kun Zhou</a><br>
    ICCV 2021 (pp. 5956-5965).
	  <br>
	  <a href="publications/ptmvs/project.html">[Project Page]</a><br>
    +:corresponding author.<br>     
	  </td>
    <td>&nbsp;</td>
  </tr>
  <tr>

    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
    <td><img src="images/scanner.jpg" width="160" height="102" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Free-form Scanning of Non-planar Appearance with Neural Trace Photography</b><br>     
    <a href="http://xiaohema98.com/">Xiaohe Ma</a>,
    <a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
    Ruisheng Zhu, 
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>+ and
    <a href="http://kunzhou.net/">Kun Zhou</a>
    <br>ACM Trans. Graph. (Proc. SIGGRAPH 2021), 40, 4 (Aug. 2021), 124.<br>
    <a href="publications/scanner/project.html">[Project Page]</a><br>
    +:corresponding author.<br>  
    </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
  &nbsp;</td>
    <td>
  &nbsp;</td>
    <td>&nbsp;</td>
      
    <tr>
    <td><img src="images/diffsurvey.jpg" width="160" height="120" class="papericon"></td>
    <td> 
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Differentiable Rendering: A Survey (in Chinese) / 可微绘制技术研究进展</b><br>     
    <a href="http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm">Weiwei Xu</a>, 
    <a href="https://zhouyangvcc.github.io/">Yang Zhou</a>, 
    <a href="http://hongzhiwu.com/">Hongzhi Wu</a> and
    <a href="http://mcg.nju.edu.cn/member/guoj/index.html">Jie Guo</a> 
    <br>Journal of Image and Graphics, 26, 6 (2021).<br>
    <a href="http://www.cjig.cn/html/jig/2021/6/20210614.htm">[Paper]</a><br>
    </td>
    <td>&nbsp;</td>
  </tr>
  <tr>

    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>    
    <tr>
    <td><img src="images/jointcap.jpg" width="160" height="145" class="papericon"></td>
    <td> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Learning Efficient Illumination Multiplexing for Joint Capture of Reflectance and Shape</b><br>     
	  <a href="http://www.cocoakang.cn">Kaizhang Kang</a>, 
	  Cihui Xie, 
	  <a href="http://cs.yale.edu/homes/che/">Chengan He</a>*,
	  Mingqi Yi*,
	  Minyi Gu,
	  Zimin Chen, 
	  <a href="http://kunzhou.net/">Kun Zhou</a> and
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>
	  <br>ACM Trans. Graph. (Proc. SIGGRAPH Asia 2019), 38, 6 (Nov. 2019), 165.<br>
      <em>(Selected as a back-cover image)</em><br>	  
	  <a href="publications/jointcap/project.html">[Project Page]</a><br>
	  *:(who) were undergraduate students when working on this project.<br> 	  
	  </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td><img src="images/autoenc.png" width="160" height="140" class="papericon"></td>
    <td> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Efficient Reflectance Capture Using an Autoencoder</b><br>     
	  <a href="http://www.cocoakang.cn">Kaizhang Kang*</a>, 
	  Zimin Chen*, 
	  <a href="http://www.jiapingwang.com/">Jiaping Wang</a>,
	  <a href="http://kunzhou.net/">Kun Zhou</a> and
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>
	  <br>ACM Trans. Graph. (Proc. SIGGRAPH 2018), 37, 4 (Aug. 2018), 127.<br>
      <a href="publications/autoenc/project.html">[Project Page]</a><br>
	  Patent# ZL201810623164.8.<br>
	  <a href="https://src.acm.org/winners/2019">2nd Place in ACM Student Research Competition, Undergraduate Group.</a><br>
	  *:Joint first authors, both of whom were undergraduate students when working on this project.	  	  
	  &nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>  
  <tr>
    <td>
	<img alt="" src="images/rgbdhair.jpg" width="160" height="77" class="papericon"></td>
	</td>
    <td>
	&nbsp;</td>
    <td class="papertext">
	<strong>Modeling Hair from an RGB-D Camera</strong><br>
	Meng Zhang, Pan Wu, <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, Yanlin Weng, <a href="http://www.youyizheng.net">Youyi Zheng</a> and <a href="http://kunzhou.net/">Kun Zhou</a><br>
	ACM Trans. Graph. (Proc. SIGGRAPH Asia 2018), 205</span>.<br>
	<span lang="en-us">
	<a href="https://dl.acm.org/citation.cfm?id=3275039">[ACM Digital Library]</a></span><br>	
    </td>
    <td>&nbsp;</td>
  </tr> 
  <tr>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>      
  <tr>
    <td>
	<img alt="" src="images/4view.jpg" width="160" height="80" class="papericon"></td>
	</td>
    <td>
	&nbsp;</td>
    <td class="papertext">
	<strong>A Data-driven Approach to Four-view Image-based Hair Modeling</strong><br>
	Meng Zhang, Menglei Chai,
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, Hao Yang and
	<a href="http://kunzhou.net/">Kun Zhou</a><br>
	ACM Trans. Graph. (Proc. SIGGRAPH 2017), 36, 4 (Jul. 2017), 156</span>.<br>
	<span lang="en-us">
	<a href="publications/4viewhair/4viewhair.pdf">[Paper(.PDF), 1.7MB]</a><a href="publications/4viewhair/4viewhair.mp4">[Video(.MP4),47.7MB]</a></span><br>	
    </td>
    <td>&nbsp;</td>
  </tr> 
  <tr>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>   
  <tr>
    <td class="style5">
	<img alt="" src="images/lf.jpg" width="160" height="120" class="papericon"></td>
	</td>
    <td>
	&nbsp;</td>
    <td class="papertext">
	<strong>Intrinsic Light Field Images</strong><br>
	Elena Garces, Jose I. Echevarria, Wen Zhang,
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>,
	<a href="http://kunzhou.net/">Kun Zhou</a>
	and <a href="http://giga.cps.unizar.es/~diegog">Diego Gutierrez</a>	
	<br>
	Computer Graphics Forum, 36(8), 589-599, Dec. 2017.<br>	
    </td>
    <td>&nbsp;</td>
  </tr> 
  <tr>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr> 
  <tr>
    <td>
	<img alt="" src="images/microlightstage.jpg" width="160" height="106" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>Simultaneous Acquisition of Microscale 
	Reflectance and Normals</strong><br>
	Giljoo Nam, Joo Ho Lee, <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>,
	<a href="http://giga.cps.unizar.es/~diegog">Diego Gutierrez</a> and
	<a href="http://www.minhkim.org">Min H. Kim</a><br>
	ACM Trans. Graph. (Proc. SIGGRAPH Asia 2016), 35, 6 (Nov. 2016), 185.<br>
	<span lang="en-us">
	<a href="publications/MicroLightStage/Microscale_SVBRDF_SIGGRAPHAsia2016.pdf">
	[Paper(.PDF), 17.8MB]</a><a href="publications/MicroLightStage/Microscale_SVBRDF_Video.mov">[Video(.MOV), 
	36.0MB]</a><a href="publications/MicroLightStage/Microscale_SVBRDF_Supplemental1.pdf">[Supp. 
	Material #1(.PDF), 0.3MB]</a><a href="publications/MicroLightStage/Microscale_SVBRDF_Supplemental2.pdf">[Supp. 
	Material #2(.PDF), 46.8MB]</a><a href="publications/MicroLightStage/bibtex.txt">[Bibtex]</a></span><a href="http://vclab.kaist.ac.kr/siggraphasia2016p2/index.html">[Project Page]</a></td>
    <td>&nbsp;</td>
  </tr> 
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img alt="" src="images/autohair.jpg" width="160" height="100" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>
	AutoHair: Fully Automatic Hair Modeling from a Single Image</strong><br>
	Menglei Chai, <a href="http://tianjiashao.com/">Tianjia Shao</a>,
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, Yanlin Weng and
	<a href="http://kunzhou.net/">Kun Zhou</a><br>
	ACM Trans. Graph. (Proc. SIGGRAPH 2016), 35, 4 (Jul. 2016), 116.<br>
	<span lang="en-us">
	<a href="publications/autohair/autohair.pdf">[Paper(.PDF), 1<span lang="zh-cn">2</span>.<span lang="zh-cn">4</span>MB]</a><a href="http://www.cad.zju.edu.cn/home/hwu/publications/autohair/autohair.mp4">[Video(.M<span lang="zh-cn">P4</span>), 
	<span lang="zh-cn">118</span>MB]</a></span><br>
	</td>
    <td>&nbsp;</td>
  </tr> 
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img alt="" src="images/imageavatar.jpg" width="160" height="100" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>
	Real-time Facial Animation with Image-based Dynamic Avatars</strong><br>
	Chen Cao,
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, Yanlin Weng, 
	<a href="http://tianjiashao.com/">Tianjia Shao</a> and
	<a href="http://kunzhou.net/">Kun Zhou</a><br>
	ACM Trans. Graph. (Proc. SIGGRAPH 2016), 35, 4 (Jul. 2016), 126.<br>
	<span lang="en-us">
	<a href="publications/avatar/avatar.pdf">[Paper(.PDF), 1<span lang="zh-cn">2</span>.<span lang="zh-cn">0</span>MB]</a><a href="http://www.cad.zju.edu.cn/home/hwu/publications/avatar/avatar.mp4">[Video(.M<span lang="zh-cn">P4</span>), 
	<span lang="zh-cn">172</span>MB]</a></span><br>
	</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img alt="" src="images/SLAE.png" width="160" height="80" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>
	Simultaneous Localization and Appearance Estimation with a Consumer RGB-D Camera</strong><br>
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, 
	Zhaotian Wang and
	<a href="http://kunzhou.net/">Kun Zhou</a><br>
	IEEE TVCG, 22, 8(Aug. 2016), 
	2012-2023.<br>
	Presented at SIGGRAPH Asia 2016 and Graphics Interface 2016.<br><a href="publications/SLAE/project.html">[Project Page]</a>
	<br>
	Patent# CN104866861, Jan. 2018.</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img alt="" src="images/RGBDcompletion.jpg" width="160" height="100" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>
	Shape Completion from a Single RGBD Image</strong><br>
	Dongping Li, <a href="http://tianjiashao.com/">Tianjia Shao</a>,
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a> and
	<a href="http://kunzhou.net/">Kun Zhou</a><br>
	IEEE TVCG, 23, 7(Jul. 2017), 1809-1822.
	<br>
	<a href="publications/ShapeCompletion/ShapeCompletion.ppsx">[Slides(.PPSX), 
      20.4MB]</a><br>
	</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img alt="" src="images/stress.png" width="160" height="111" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>
	Stress Constrained Thickness Optimization for Shell Object Fabrication</strong><br>
	Haiming Zhao, Weiwei Xu, <a href="http://kunzhou.net">Kun Zhou</a>, 
	<a href="http://www.unm.edu/~yangy/">Yin Yang</a>,
	<a href="http://www.cad.zju.edu.cn/home/jin/">Xiaogang Jin</a> and <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a><br>
	Computer Graphics Forum, 36: 368-380, Sep. 2017.<br>
	</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img src="images/appfusion.jpg" width="160" height="82" class="papericon"></td>
    <td>
	</td>	
    <td class="papertext"><strong>
	AppFusion: Interactive Appearance Acquisition using 
	a Kinect Sensor</strong><br>
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a> and
	<a href="http://kunzhou.net/">Kun Zhou</a><br>
	Computer Graphics Forum, 34(6), 289-298, Sep. 2015.<br>
	<a href="publications/AppFusion/appfusion.pdf">[Paper(.PDF), 
	10.2MB]</a>
	<a href="publications/AppFusion/appfusion.mp4">[Video(.MP4), 49.1MB]</a>
	<span lang="en-us"><a href="publications/AppFusion/AppFusion.ppsx">
	[Presentation(.PPSX), 79.5MB]</a></span>
	<a href="publications/AppFusion/appfusion.txt">[BibTex]</a>
	<br>
	Patent# CN103955958, Aug. 2016.</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td>
	<img alt="" src="publications/btf_percept/btf_percept.png" width="160" height="81" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>
	Effects of Approximate Filtering on the Appearance of Bidirectional Texture Functions</strong><br>
      <a href="http://giga.cps.unizar.es/~ajarabo/">Adrian Jarabo</a>,
	<a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, 
	<a href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a>,
	<a href="http://graphics.cs.yale.edu/holly/">Holly Rushmeier</a> and 
	<a href="http://giga.cps.unizar.es/~diegog">Diego Gutierrez</a><br>
	IEEE TVCG, 20, 6(Jun. 2014), 880-892.<br><a href="publications/btf_percept/jarabo_tvcg14.pdf">[Paper(.PDF), 
	9.1MB]</a> <a href="publications/btf_percept/jarabo_tvcg14sup.pdf">[Supplementary 
	Material(.PDF), 21.3MB]</a>
	<a href="publications/btf_percept/btf_percept_bibtex.txt">[BibTex]</a><br>
	</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>  
  <tr>
    <td>
	<img src="images/inverse.jpg" width="160" height="120" class="papericon"></td>
    <td>
	</td>
    <td class="papertext"><strong>Inverse Bi-scale Material Design</strong><br>
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, <a href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a> and <a href="http://graphics.cs.yale.edu/holly/">Holly Rushmeier</a><br>
	ACM Trans. Graph. (Proc. SIGGRAPH Asia
	2013), 32, 6 (Nov. 2013), 163:1-163:10.<br>
	<em>(Selected for the <a href="http://www.youtube.com/watch?v=FUGVF_eMeo4">SIGGRAPH Asia movie trailer</a>)</em><br>
      <a href="publications/InvBiscale/project.html">[Project Page]</a><br>
	</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td ><img src="images/BiScale.jpg" width="160" height="137" class="papericon"></td>
    <td>
	</td>    
    <td class="papertext"><strong>Physically-based Interactive Bi-scale Material Design</strong><br>
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, <a href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a> and <a href="http://graphics.cs.yale.edu/holly/">Holly Rushmeier</a><br>
      ACM Trans. Graph. (Proc. SIGGRAPH Asia 2011), 30, 6 (Dec. 2011), 145:1-145:10. <br>
      <em>(Selected as a back-cover image)</em><br>
      <a href="publications/BiScale/biscale.pdf">[Paper(.PDF), 8.5MB]</a> <a href="publications/BiScale/biscale.mp4">[Video(.MP4), 30.6MB]</a> <a href="http://www.youtube.com/watch?v=C89dEG921Ys">[YouTube]</a> <a href="publications/BiScale/BiScale.ppsx">[SIGAsia Slides(.PPSX), 
      29.6MB]</a> <a href="publications/BiScale/biscale_bibtex.txt">[BibTex]</a></td>
    <td width="6%">&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>  
  <tr>
    <td><img src="images/SPMM.jpg" width="160" height="120" class="papericon"></td>
    <td>
	</td>    
    <td class="papertext"><strong>A Sparse Parametric Mixture Model for BTF Compression, Editing and Rendering</strong><br>    
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, <a href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a> and <a href="http://graphics.cs.yale.edu/holly/">Holly Rushmeier</a><br>
      Computer Graphics Forum (Proc. EUROGRAPHICS 2011), 30(2), 465-473.<br>      
      <a href="publications/SPMM/SPMM.pdf">[Paper(.PDF), 4.9MB]</a> <a href="publications/SPMM/SPMM.zip">[Video(.ZIP), 21.4MB]</a> <a href="http://www.youtube.com/watch?v=B5lARr4ZGiY">[YouTube]</a> <a href="publications/SPMM/EG_Presentation.ppsx">[EG Slides(.PPSX), 
	  4.7MB]</a> <a href="publications/SPMM/bibtex.txt">[BibTex]</a></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>  
  <tr>
    <td><img src="images/CPM.jpg" width="160" height="120" class="papericon"></td>
    <td>
	</td>    
    <td class="papertext"><strong>Characteristic Point Maps</strong><br>
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, <a href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a> and <a href="http://graphics.cs.yale.edu/holly/">Holly Rushmeier</a><br>
      Computer Graphics Forum (Proc. EUROGRAPHICS Symposium on Rendering, 2009), 28(4), 1227-1236.<br>      
      <a
      href="publications/CPM/CPM.pdf">[Paper(.PDF), 11.2MB]</a> <a
      href="publications/CPM/CPM.zip">[EGSR Slides(.ZIP) 2.6MB]</a> <a
      href="publications/CPM/CPM_bibtex.txt" target="_blank">[BibTex]</a></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>  
  <tr>
    <td><img src="images/CAT.jpg" width="160" height="120" class="papericon"></td>
    <td>
	</td>    
    <td class="papertext"><strong>Context-Aware
      Textures</strong><br>      
      <a
      href="http://graphics.cs.yale.edu/jianye/">Jianye Lu</a>, <a href="http://cvc.yale.edu/people/Athos.html">Athinodoros S. Georghiades</a>,
        Andreas Glaser, <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, <a
      href="http://graphics.stanford.edu/~liyiwei/">Li-Yi Wei</a>, <a
      href="http://research.microsoft.com/users/bainguo/">Baining Guo</a>, <a
      href="http://graphics.cs.yale.edu/julie/">Julie Dorsey</a> and <a
      href="http://graphics.cs.yale.edu/holly/">Holly Rushmeier</a><br> 
        ACM
        Trans. Graph. 26, 1(Jan. 2007), 3. <br>
        <i> (Selected as the cover image)</i><br>      
      <a href="publications/CAT/TOG_CAT.pdf">[Paper(.PDF), 3.6MB]</a> <a href="publications/CAT/TOG_CAT.avi">[Video(.AVI), 22.6MB]</a> <a href="http://www.youtube.com/watch?v=kDg3pdHw3Go">[Youtube]</a> <a
      href="publications/CAT/CAT_bibtex.txt" target="_blank">[BibTex]</a><a href="http://graphics.cs.yale.edu/CAT/">[Project Webpage]</a></td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td><img src="images/SilTex.jpg" width="160" height="120" class="papericon"></td>
    <td> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Silhouette
      Texture</b><br>      
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, <a href="http://graphics.stanford.edu/~liyiwei/">Li-Yi Wei</a>,
        Xi Wang and <a href="http://research.microsoft.com/users/bainguo/">Baining Guo</a><br>Rendering technique (Proc. EUROGRAPHICS
      Symposium on Rendering, 2006), 285-296.<br>
      <a
      href="publications/siltex/siltex.pdf">[Paper(.PDF), 4.1MB]</a> <a
      href="publications/siltex/siltex.mov">[Video(.MOV), 44MB]</a> <a href="http://www.youtube.com/watch?v=GE1pE6yNU3Q">[YouTube]</a> <a href="publications/siltex/SilTex.ppsx">[EGSR Slides(.PPSX), 12.3MB]</a> <a
      href="publications/siltex/SilTex_bibtex.txt" target="_blank">[BibTex]</a>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td>
	&nbsp;</td>
    <td class="papertext">&nbsp;</td>
    <td>&nbsp;</td>
  </tr>    
  <tr>
    <td><img src="images/book.jpg" width="160" height="103" class="papericon"></td>
    <td> 
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>    
    <td class="papertext"><b>Computer Graphics: Principles and Practices, 3rd Ed.</b><br>    
	  John F. Hugues, Andries van Dam, Morgan McGuire, David F. Sklar, James D. Foley, Steven K. Feiner and Kurt Akeley, Addison-Wesley, 2013.<br>
	  Translator for the Official Chinese Version.
	  <br>      
	  [Basic Topics] by 
	  <a href="http://www.cad.zju.edu.cn/home/peng/">Qunsheng Peng</a>, 
	  <a href="http://www.cad.zju.edu.cn/home/xgliu/">Xinguo Liu</a>, 
	  Lanfang Miao
	  and
      <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>, China Machine Press, 2018.
	  <a href="https://item.jd.com/12462129.html">[Online Store]</a>
	  <br>      
	  [Advanced Topics] by 
	  <a href="http://www.cad.zju.edu.cn/home/peng/">Qunsheng Peng</a>, 
	  <a href="http://www.cad.zju.edu.cn/home/hwu/">Hongzhi Wu</a>,
	  <a href="http://www.cad.zju.edu.cn/home/rwang/">Rui Wang</a> and 
	  <a href="http://www.cad.zju.edu.cn/home/xgliu/">Xinguo Liu</a>, China Machine Press, 2021.
	  <a href="https://item.jd.com/13098614.html">[Online Store]</a>
	  <br>      
	  &nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</table>
	<br>

<p class="textsectionheader2">Alumni</p>
  Xiaohe Ma (PhD 2025/Meshy)<br>
  Chong Zeng (MEng 2025/Stanford University)<br>
  Yixin Zeng (MEng 2025/Carnegie Mellon University)<br>
  Xiang Feng (MEng 2025/UCSD)<br>
  Zoubin Bi (MEng 2025/Youku)<br>
  Xianmin Xu (MEng 2024/Huawei)<br>
  Leyao Zhang (BEng 2024/Georgia Tech)<br>
  Mingrui Yin (BEng 2024/UCSD)<br>
  Kaizhang Kang (PhD 2023/KAUST/Microsoft Research Asia Fellowship)<br>
  Jinjiang You (BEng 2023/Carnegie Mellon University)<br>
  Peipei Zhong (BEng 2023/Carnegie Mellon University)<br>
  Kangping Hu (BEng 2023/Georgia Tech)<br>
  Yaxin Yu (MEng 2023/Alibaba)<br>
  Mingqi Yi (MEng 2022/MiHoYo)<br>
  Ruisheng Zhu (MEng 2022/Huawei)<br>
  Zirong Kang (MEng 2022/Alibaba)<br>
  Haoyang Zhou (BEng 2022/ETH)<br>
  Qingyi He (BEng 2022/EPFL)<br>
	Minyi Gu (MEng 2021/Tencent)<br>
	Zimen Chen (MEng 2021/Tencent)<br>
	Xuanda Yang (BEng 2020/UCSD)<br>
	Ren Liu (BEng 2020/Georgia Tech)<br>
	Xiaofeng Zeng (BEng 2020/KTH)<br>
	Yaxin Peng (BEng 2020/UIUC)<br>
	Cihui Xie (MEng 2020/FaceUnity)<br>
	Chengan He (BEng 2019/Yale University)<br>
	Zhongshu Du (BEng 2019/UCSD)<br>
	Wanpeng Li (MEng 2018/Microsoft/Excellent Graduate Award)<br>
	Yiwei Hu (BEng 2018/Yale University)<br>
	Wenjia Zhang (BEng 2018/Yale University)<br>	
	Meng Yuan (BSc 2018/Northwestern University)<br>	
	Xin Hao (MEng 2018/NetEase Games)<br>
	Zhenyu Tang (BEng 2017/UNC Chapel Hill)</span><br>
	Tingting Chen (MEng 2017/Microsoft)<br>
	Liran Qin (MEng 2017/YituTech)<br>
	Wen Zhang (BEng 2016/UCSD)<br>
	Zhaotian Wang (MEng 2016/Google USA/Excellent Graduate Award)<br>
	<br>
	
	<p class="textsectionheader2">Note</p>
  I write recommendation letters for students that closely work with me on one or more research/engineering projects.
	For students who take my courses only, I cannot write the letters due to highly limited interactions.<br>
	</div>
</body>
</html>
